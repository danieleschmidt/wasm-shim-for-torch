#!/usr/bin/env python3
"""
Autonomous Self-Healing Test Suite for WASM-Torch
Enhanced with AI-powered test generation and quantum validation
"""

import asyncio
import json
import time
import logging
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from wasm_torch.autonomous_testing_framework import (
    get_test_framework, 
    TestCase, 
    TestType,
    autonomous_test
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class AutonomousSelfHealingTestSuite:
    """Enhanced autonomous test suite with self-healing capabilities"""
    
    def __init__(self):
        self.framework = get_test_framework()
        self.test_results: Dict[str, Any] = {}
        self.start_time = time.time()
        
    async def initialize(self):
        """Initialize the test suite"""
        logger.info("üöÄ Initializing Autonomous Self-Healing Test Suite...")
        
        # Register core test functions
        await self._register_core_tests()
        
        # Generate AI-powered tests
        await self._generate_ai_tests()
        
        logger.info(f"‚úÖ Initialized with {len(self.framework.test_registry)} tests")
    
    async def _register_core_tests(self):
        """Register core functionality tests"""
        
        @autonomous_test(TestType.UNIT, priority=10)
        async def test_basic_import():
            """Test basic module imports"""
            try:
                import wasm_torch
                return "Import successful"
            except ImportError as e:
                return f"Import failed: {e}"
        
        @autonomous_test(TestType.UNIT, priority=9)
        async def test_mock_torch_functionality():
            """Test mock torch implementation"""
            try:
                from wasm_torch.mock_torch import MockTensor
                tensor = MockTensor([1, 2, 3])
                return f"Mock tensor created: {tensor.shape}"
            except Exception as e:
                return f"Mock torch error: {e}"
        
        @autonomous_test(TestType.INTEGRATION, priority=8)
        async def test_wasm_runtime_basic():
            """Test basic WASM runtime functionality"""
            try:
                from wasm_torch import WASMRuntime
                runtime = WASMRuntime()
                if hasattr(runtime, 'init'):
                    await runtime.init()
                return "Runtime initialized successfully"
            except Exception as e:
                return f"Runtime error: {e}"
        
        @autonomous_test(TestType.SECURITY, priority=7)
        async def test_security_validation():
            """Test security validation systems"""
            try:
                from wasm_torch.security import SecurityManager
                security_mgr = SecurityManager()
                return "Security manager initialized"
            except Exception as e:
                return f"Security error: {e}"
        
        @autonomous_test(TestType.PERFORMANCE, priority=6)
        async def test_performance_monitoring():
            """Test performance monitoring capabilities"""
            try:
                from wasm_torch.monitoring import PerformanceMonitor
                monitor = PerformanceMonitor()
                return f"Performance monitor initialized: {type(monitor)}"
            except Exception as e:
                return f"Performance monitoring error: {e}"
        
        @autonomous_test(TestType.RELIABILITY, priority=5)
        async def test_error_recovery():
            """Test error recovery mechanisms"""
            try:
                from wasm_torch.error_recovery import ErrorRecoverySystem
                recovery = ErrorRecoverySystem()
                return "Error recovery system initialized"
            except Exception as e:
                return f"Error recovery error: {e}"
        
        logger.info("‚úÖ Core tests registered")
    
    async def _generate_ai_tests(self):
        """Generate AI-powered tests for discovered functions"""
        try:
            # Import main modules and generate tests
            import wasm_torch
            
            # Generate tests for export function
            if hasattr(wasm_torch, 'export_to_wasm'):
                await self.framework.generate_tests_for_function(wasm_torch.export_to_wasm)
            
            # Generate tests for runtime
            if hasattr(wasm_torch, 'WASMRuntime'):
                runtime_class = wasm_torch.WASMRuntime
                if hasattr(runtime_class, '__init__'):
                    await self.framework.generate_tests_for_function(runtime_class.__init__)
            
            logger.info("‚úÖ AI-powered tests generated")
            
        except Exception as e:
            logger.warning(f"AI test generation failed: {e}")
    
    async def run_comprehensive_test_suite(self) -> Dict[str, Any]:
        """Run the complete autonomous test suite"""
        logger.info("üß™ Starting Comprehensive Autonomous Test Suite...")
        
        # Run all tests with self-healing enabled
        results = await self.framework.run_test_suite()
        
        # Add self-healing metrics
        healing_metrics = self._calculate_healing_metrics()
        results["self_healing_metrics"] = healing_metrics
        
        # Add adaptive intelligence metrics
        ai_metrics = self._calculate_ai_metrics()
        results["ai_metrics"] = ai_metrics
        
        self.test_results = results
        return results
    
    def _calculate_healing_metrics(self) -> Dict[str, Any]:
        """Calculate self-healing performance metrics"""
        healing_success_rate = (
            sum(self.framework.healing_success_rate) / len(self.framework.healing_success_rate)
            if self.framework.healing_success_rate else 0.0
        )
        
        return {
            "healing_success_rate": healing_success_rate * 100,
            "total_healing_attempts": len(self.framework.healing_success_rate),
            "adaptation_metrics": dict(self.framework.adaptation_metrics),
            "failure_patterns": {
                pattern: len(errors) 
                for pattern, errors in self.framework.failure_patterns.items()
            },
            "healing_strategies_available": len(self.framework.healing_strategies)
        }
    
    def _calculate_ai_metrics(self) -> Dict[str, Any]:
        """Calculate AI-powered testing metrics"""
        ai_generated_tests = [
            test for test in self.framework.test_registry.values()
            if "ai_generated" in test.tags
        ]
        
        quantum_validated_tests = [
            execution for execution in self.framework.execution_history
            if "confidence" in execution.metrics
        ]
        
        avg_quantum_confidence = (
            sum(ex.metrics["confidence"] for ex in quantum_validated_tests) / len(quantum_validated_tests)
            if quantum_validated_tests else 0.0
        )
        
        return {
            "ai_generated_tests": len(ai_generated_tests),
            "quantum_validated_tests": len(quantum_validated_tests),
            "avg_quantum_confidence": avg_quantum_confidence * 100,
            "ai_generation_enabled": self.framework.enable_ai_generation,
            "quantum_validation_enabled": self.framework.enable_quantum_validation
        }
    
    async def run_continuous_testing(self, duration_hours: float = 24.0):
        """Run continuous testing for specified duration"""
        logger.info(f"üîÑ Starting continuous testing for {duration_hours} hours...")
        
        # Start continuous testing
        await self.framework.continuous_testing(interval=1800)  # Every 30 minutes
        
        # Wait for specified duration
        await asyncio.sleep(duration_hours * 3600)
        
        # Stop continuous testing
        await self.framework.stop_continuous_testing()
        
        logger.info("‚úÖ Continuous testing completed")
    
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """Generate comprehensive test report"""
        execution_time = time.time() - self.start_time
        framework_status = self.framework.get_test_status()
        
        report = {
            "metadata": {
                "suite_version": "2.0.0",
                "execution_time": execution_time,
                "timestamp": time.time(),
                "framework": "Autonomous Self-Healing Test Framework"
            },
            "test_results": self.test_results,
            "framework_status": framework_status,
            "capabilities": {
                "self_healing": True,
                "ai_generation": self.framework.enable_ai_generation,
                "quantum_validation": self.framework.enable_quantum_validation,
                "parallel_execution": self.framework.parallel_execution,
                "continuous_testing": self.framework.is_running
            },
            "recommendations": self._generate_recommendations()
        }
        
        return report
    
    def _generate_recommendations(self) -> List[str]:
        """Generate recommendations based on test results"""
        recommendations = []
        
        if not self.test_results:
            return ["Run test suite first to generate recommendations"]
        
        pass_rate = self.test_results.get("summary", {}).get("pass_rate", 0)
        
        if pass_rate < 70:
            recommendations.append("‚ùó Low pass rate detected - consider increasing self-healing aggressiveness")
        elif pass_rate < 90:
            recommendations.append("‚ö†Ô∏è Moderate pass rate - monitor failing tests for patterns")
        else:
            recommendations.append("‚úÖ Excellent pass rate - system is stable")
        
        # Healing recommendations
        healing_metrics = self.test_results.get("self_healing_metrics", {})
        healing_rate = healing_metrics.get("healing_success_rate", 0)
        
        if healing_rate > 80:
            recommendations.append("‚úÖ Self-healing system performing excellently")
        elif healing_rate > 50:
            recommendations.append("‚ö†Ô∏è Self-healing system needs optimization")
        else:
            recommendations.append("‚ùó Self-healing system requires attention")
        
        # Performance recommendations
        avg_execution_time = self.test_results.get("summary", {}).get("avg_test_time", 0)
        if avg_execution_time > 5.0:
            recommendations.append("‚ö†Ô∏è Tests running slowly - consider optimization")
        
        # AI recommendations
        ai_metrics = self.test_results.get("ai_metrics", {})
        if ai_metrics.get("ai_generated_tests", 0) == 0:
            recommendations.append("üí° Enable AI test generation for better coverage")
        
        return recommendations

async def main():
    """Main execution function"""
    print("üöÄ AUTONOMOUS SELF-HEALING TEST SUITE v2.0")
    print("=" * 60)
    
    # Initialize test suite
    suite = AutonomousSelfHealingTestSuite()
    await suite.initialize()
    
    try:
        # Run comprehensive test suite
        results = await suite.run_comprehensive_test_suite()
        
        # Generate and display report
        report = suite.generate_comprehensive_report()
        
        # Display results
        print("\nüìä TEST EXECUTION SUMMARY:")
        summary = results.get("summary", {})
        print(f"   Total Tests: {summary.get('total_tests', 0)}")
        print(f"   Passed: {summary.get('passed', 0)}")
        print(f"   Failed: {summary.get('failed', 0)}")
        print(f"   Pass Rate: {summary.get('pass_rate', 0):.1f}%")
        print(f"   Execution Time: {summary.get('total_execution_time', 0):.2f}s")
        
        # Display self-healing metrics
        healing_metrics = results.get("self_healing_metrics", {})
        print(f"\nüîß SELF-HEALING METRICS:")
        print(f"   Healing Success Rate: {healing_metrics.get('healing_success_rate', 0):.1f}%")
        print(f"   Total Healing Attempts: {healing_metrics.get('total_healing_attempts', 0)}")
        print(f"   Strategies Available: {healing_metrics.get('healing_strategies_available', 0)}")
        
        # Display AI metrics
        ai_metrics = results.get("ai_metrics", {})
        print(f"\nüß† AI TESTING METRICS:")
        print(f"   AI Generated Tests: {ai_metrics.get('ai_generated_tests', 0)}")
        print(f"   Quantum Validated: {ai_metrics.get('quantum_validated_tests', 0)}")
        print(f"   Avg Quantum Confidence: {ai_metrics.get('avg_quantum_confidence', 0):.1f}%")
        
        # Display recommendations
        recommendations = report.get("recommendations", [])
        if recommendations:
            print(f"\nüí° RECOMMENDATIONS:")
            for rec in recommendations:
                print(f"   {rec}")
        
        # Save detailed report
        report_file = Path("autonomous_self_healing_test_report.json")
        with open(report_file, "w") as f:
            json.dump(report, f, indent=2, default=str)
        
        print(f"\nüìÅ Detailed report saved to: {report_file}")
        
        # Determine overall status
        pass_rate = summary.get("pass_rate", 0)
        healing_rate = healing_metrics.get("healing_success_rate", 0)
        
        if pass_rate >= 90 and healing_rate >= 80:
            status = "EXCELLENT"
            emoji = "üöÄ"
        elif pass_rate >= 75 and healing_rate >= 60:
            status = "GOOD"
            emoji = "‚úÖ"
        elif pass_rate >= 50:
            status = "NEEDS_IMPROVEMENT"
            emoji = "‚ö†Ô∏è"
        else:
            status = "CRITICAL"
            emoji = "‚ùå"
        
        print(f"\n{emoji} OVERALL STATUS: {status}")
        print("=" * 60)
        
        return 0 if status in ["EXCELLENT", "GOOD"] else 1
        
    except Exception as e:
        logger.error(f"Test suite execution failed: {e}")
        print(f"\n‚ùå EXECUTION FAILED: {e}")
        return 1

if __name__ == "__main__":
    try:
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\nüõë Test suite interrupted by user")
        sys.exit(130)
    except Exception as e:
        print(f"\nüí• Fatal error: {e}")
        sys.exit(1)